---
- name: Deploy Observability Stack (EFK + Jaeger)
  hosts: master
  become: no
  gather_facts: no

  environment:
    KUBECONFIG: /home/ubuntu/.kube/config

  tasks:
    # ====================================================================
    # SECTION 1: EFK LOGGING STACK
    # ====================================================================

    - name: 1. Create Logging Namespace
      kubernetes.core.k8s:
        name: logging
        api_version: v1
        kind: Namespace
        state: present

    - name: 2. Deploy Elasticsearch (Database)
      kubernetes.core.k8s:
        state: present
        namespace: logging
        definition: "{{ lookup('file', 'elasticsearch.yaml') }}"

    - name: 3. Deploy Kibana (Dashboard)
      kubernetes.core.k8s:
        state: present
        namespace: logging
        definition: "{{ lookup('file', 'kibana.yaml') }}"

    - name: 4. Deploy Fluent Bit (Collector)
      kubernetes.core.k8s:
        state: present
        namespace: logging
        definition: "{{ lookup('file', 'fluent-bit.yaml') }}"

    - name: 5. Restart Fluent Bit DaemonSet (To apply configs)
      ansible.builtin.shell:
        cmd: "kubectl rollout restart daemonset fluent-bit -n logging"

    - name: 6a. Check if metrics-server deployment exists
      ansible.builtin.shell: |
        kubectl get deployment metrics-server -n kube-system
      register: metrics_server_check
      failed_when: false
      changed_when: false

    - name: 6b. Patch Metrics Server (Stop certificate verification spam)
      ansible.builtin.shell: |
        kubectl patch deployment metrics-server -n kube-system --type='json' \
          -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'
      when: metrics_server_check.rc == 0

    # ====================================================================
    # SECTION 2: KIBANA CONFIGURATION
    # ====================================================================

    - name: 7a. Wait for Elasticsearch to be Ready
      kubernetes.core.k8s_info:
        kind: Pod
        name: es-cluster-0
        namespace: logging
      register: es_pod_status
      until: es_pod_status.resources[0].status.phase == 'Running'
      retries: 20
      delay: 15

    # -------------------------------------------------------
    # 1. STRICT WAIT: Loop until Kibana Status is actually 'Green'
    # -------------------------------------------------------
    - name: 7b. Wait for Kibana Status to be 'Green' (Available)
      shell: |
        kubectl run wait-for-kibana --image=curlimages/curl --restart=Never --rm -i -- \
          curl -s "http://kibana.logging.svc.cluster.local:5601/api/status"
      register: kibana_status
      # Keep retrying until the word "available" or "green" appears in the JSON response
      until: 
        - "'available' in kibana_status.stdout or 'green' in kibana_status.stdout"
      retries: 60       # Try for 10 minutes (30 * 10s)
      delay: 10
      changed_when: false
      failed_when: false # Don't fail if curl errors initially (pod starting)

    - name: Sleep for 45 seconds to let Kibana initialize fully
        ansible.builtin.pause:
          seconds: 45

    # -------------------------------------------------------
    # 2. CREATE INDEX: Now safe to run because Kibana is Green
    # -------------------------------------------------------
    - name: 7c. Create 'logstash-*' Index Pattern
      shell: |
        kubectl run create-index --image=curlimages/curl --restart=Never --rm -i -- \
          curl -s -X POST "http://kibana.logging.svc.cluster.local:5601/api/saved_objects/index-pattern/logstash-pattern" \
          -H "kbn-xsrf: true" \
          -H "Content-Type: application/json" \
          -d '{"attributes": {"title": "logstash-*", "timeFieldName": "@timestamp"}}'
      register: kibana_index_result
      # Retry thiss specific step just in case of momentary API blips
      retries: 5
      delay: 5
      until: kibana_index_result.rc == 0
      # Valid failure: Exit code non-zero AND it wasn't a "409 Conflict" (already exists)
      failed_when: 
        - kibana_index_result.rc != 0
        - "'409' not in kibana_index_result.stdout"
      changed_when: "'200' in kibana_index_result.stdout"

    # -------------------------------------------------------
    # 3. SET DEFAULT
    # -------------------------------------------------------
    - name: 7d. Set Default Index Pattern to 'logstash-*'
      shell: |
        kubectl run set-default-index --image=curlimages/curl --restart=Never --rm -i -- \
          curl -s -X POST "http://kibana.logging.svc.cluster.local:5601/api/kibana/settings" \
          -H "kbn-xsrf: true" \
          -H "Content-Type: application/json" \
          -d '{"changes": {"defaultIndex": "logstash-pattern"}}'
      register: default_index_result
      retries: 5
      delay: 5
      until: default_index_result.rc == 0
      changed_when: "'200' in default_index_result.stdout"
      failed_when: 
        - default_index_result.rc != 0
        - "'200' not in default_index_result.stdout"

    # ====================================================================
    # SECTION 3: JAEGER TRACING
    # ====================================================================

    - name: 8. Deploy Jaeger (Service & Ingress)
      kubernetes.core.k8s:
        state: present
        namespace: logging
        definition: "{{ lookup('file', 'jaeger.yaml') }}"

    - name: 9. Deploy deployment yaml
      kubernetes.core.k8s:
        state: present
        namespace: ticketing-app
        definition: "{{ lookup('file', 'active-deployment.yaml') }}"

    - name: Final Summary
      ansible.builtin.debug:
        msg:
          - "-----------------------------------------"
          - "ðŸŽ‰ Observability Deployment Complete!"
          - "âœ… EFK Stack Deployed"
          - "âœ… Kibana Index Pattern Created"
          - "âœ… Jaeger Deployed"
          - "âœ… App Updated to v5 with Tracing"
          - "-----------------------------------------"
